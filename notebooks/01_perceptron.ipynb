{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. 퍼셉트론 (Perceptron)\n",
    "\n",
    "가장 간단한 신경망인 퍼셉트론을 구현하고 이해합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 퍼셉트론의 구조와 작동 원리 이해\n",
    "- 경사하강법(Gradient Descent) 이해\n",
    "- 이진 분류 문제 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 시드 고정\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 퍼셉트론이란?\n",
    "\n",
    "퍼셉트론은 가장 간단한 형태의 인공 뉴런입니다.\n",
    "\n",
    "$$y = \\text{sign}(w \\cdot x + b)$$\n",
    "\n",
    "- $x$: 입력 벡터\n",
    "- $w$: 가중치 벡터\n",
    "- $b$: 편향(bias)\n",
    "- $\\text{sign}$: 부호 함수 (양수면 1, 음수면 -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 생성\n",
    "\n",
    "선형 분리 가능한 2D 데이터를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "n_samples = 100\n",
    "\n",
    "# 클래스 1: 중심 (2, 2)\n",
    "class1 = torch.randn(n_samples // 2, 2) + torch.tensor([2.0, 2.0])\n",
    "# 클래스 2: 중심 (-2, -2)\n",
    "class2 = torch.randn(n_samples // 2, 2) + torch.tensor([-2.0, -2.0])\n",
    "\n",
    "X = torch.cat([class1, class2], dim=0)\n",
    "y = torch.cat([torch.ones(n_samples // 2), -torch.ones(n_samples // 2)])\n",
    "\n",
    "# 데이터 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Class 1', alpha=0.7)\n",
    "plt.scatter(X[y == -1, 0], X[y == -1, 1], c='red', label='Class -1', alpha=0.7)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('Binary Classification Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 퍼셉트론 직접 구현\n",
    "\n",
    "PyTorch의 autograd를 활용하지 않고, 퍼셉트론 학습 규칙을 직접 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptronManual:\n",
    "    \"\"\"\n",
    "    퍼셉트론 (수동 구현)\n",
    "    \n",
    "    퍼셉트론 학습 규칙:\n",
    "    - 예측이 맞으면: 가중치 업데이트 없음\n",
    "    - 예측이 틀리면: w = w + lr * y * x\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, lr: float = 0.1):\n",
    "        self.weight = torch.zeros(input_size)\n",
    "        self.bias = torch.zeros(1)\n",
    "        self.lr = lr\n",
    "    \n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"예측: sign(w·x + b)\"\"\"\n",
    "        linear = x @ self.weight + self.bias\n",
    "        return torch.sign(linear)\n",
    "    \n",
    "    def fit(self, X: torch.Tensor, y: torch.Tensor, epochs: int = 100):\n",
    "        \"\"\"학습\"\"\"\n",
    "        n_samples = X.size(0)\n",
    "        errors_per_epoch = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            errors = 0\n",
    "            for i in range(n_samples):\n",
    "                xi, yi = X[i], y[i]\n",
    "                prediction = self.predict(xi)\n",
    "                \n",
    "                # 예측이 틀리면 업데이트\n",
    "                if prediction != yi:\n",
    "                    self.weight += self.lr * yi * xi\n",
    "                    self.bias += self.lr * yi\n",
    "                    errors += 1\n",
    "            \n",
    "            errors_per_epoch.append(errors)\n",
    "            \n",
    "            # 에러가 0이면 조기 종료\n",
    "            if errors == 0:\n",
    "                print(f\"Converged at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        return errors_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론 학습\n",
    "perceptron = PerceptronManual(input_size=2, lr=0.1)\n",
    "errors = perceptron.fit(X, y, epochs=100)\n",
    "\n",
    "# 학습 결과 시각화\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(errors)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.title('Training Errors per Epoch')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# 결정 경계 시각화\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Class 1', alpha=0.7)\n",
    "plt.scatter(X[y == -1, 0], X[y == -1, 1], c='red', label='Class -1', alpha=0.7)\n",
    "\n",
    "# 결정 경계: w1*x1 + w2*x2 + b = 0\n",
    "w = perceptron.weight\n",
    "b = perceptron.bias\n",
    "x1_range = torch.linspace(-5, 5, 100)\n",
    "x2_boundary = -(w[0] * x1_range + b) / (w[1] + 1e-10)\n",
    "plt.plot(x1_range, x2_boundary, 'g-', linewidth=2, label='Decision Boundary')\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend()\n",
    "plt.title('Learned Decision Boundary')\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-5, 5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. mlfs 패키지 사용\n",
    "\n",
    "이제 우리가 구현한 `mlfs` 패키지를 사용해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.nn.models import Perceptron\n",
    "from mlfs.nn.optim import SGD\n",
    "from mlfs.nn.losses import BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블을 0, 1로 변환 (BCE 손실용)\n",
    "y_binary = (y + 1) / 2  # -1, 1 -> 0, 1\n",
    "\n",
    "# 모델 생성\n",
    "model = Perceptron(input_size=2)\n",
    "optimizer = SGD(model.parameters(), lr=0.1)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "# 학습\n",
    "losses = []\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 순전파\n",
    "    logits = model(X)\n",
    "    loss = criterion(logits, y_binary)\n",
    "    \n",
    "    # 역전파\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        predictions = model.predict(X)\n",
    "        accuracy = (predictions == y_binary.long()).float().mean()\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "from mlfs.utils.viz import plot_loss\n",
    "\n",
    "plot_loss(losses, title='Training Loss (mlfs Perceptron)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MNIST로 확장\n",
    "\n",
    "실제 데이터셋인 MNIST에서 이진 분류를 수행해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.utils.data import load_mnist\n",
    "\n",
    "# MNIST 로드\n",
    "X_train, y_train = load_mnist(train=True, flatten=True)\n",
    "X_test, y_test = load_mnist(train=False, flatten=True)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 분류: 0 vs 1\n",
    "def filter_binary(X, y, class0=0, class1=1):\n",
    "    mask = (y == class0) | (y == class1)\n",
    "    X_filtered = X[mask]\n",
    "    y_filtered = y[mask]\n",
    "    y_binary = (y_filtered == class1).float()\n",
    "    return X_filtered, y_binary\n",
    "\n",
    "X_train_bin, y_train_bin = filter_binary(X_train, y_train, 0, 1)\n",
    "X_test_bin, y_test_bin = filter_binary(X_test, y_test, 0, 1)\n",
    "\n",
    "print(f\"Binary Train: {X_train_bin.shape}\")\n",
    "print(f\"Binary Test: {X_test_bin.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퍼셉트론 학습\n",
    "model = Perceptron(input_size=784)\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "\n",
    "losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 미니배치 학습\n",
    "    epoch_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    indices = torch.randperm(len(X_train_bin))\n",
    "    \n",
    "    for i in range(0, len(X_train_bin), batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = X_train_bin[batch_idx]\n",
    "        y_batch = y_train_bin[batch_idx]\n",
    "        \n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    with torch.no_grad():\n",
    "        train_pred = model.predict(X_train_bin)\n",
    "        train_acc = (train_pred == y_train_bin.long()).float().mean().item()\n",
    "        \n",
    "        test_pred = model.predict(X_test_bin)\n",
    "        test_acc = (test_pred == y_test_bin.long()).float().mean().item()\n",
    "    \n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {avg_loss:.4f}, Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accs, label='Train')\n",
    "axes[1].plot(test_accs, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 노트북에서 배운 내용:\n",
    "\n",
    "1. **퍼셉트론 구조**: 입력 → 가중치 합 → 활성화 함수 → 출력\n",
    "2. **퍼셉트론 학습 규칙**: 틀린 예측에 대해서만 가중치 업데이트\n",
    "3. **경사하강법**: 손실 함수를 최소화하는 방향으로 파라미터 업데이트\n",
    "4. **한계**: 선형 분리 불가능한 문제(XOR)는 해결 불가\n",
    "\n",
    "다음 노트북에서는 이 한계를 극복하는 **다층 퍼셉트론(MLP)**을 학습합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
