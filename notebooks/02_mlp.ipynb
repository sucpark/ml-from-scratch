{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 다층 퍼셉트론 (Multi-Layer Perceptron)\n",
    "\n",
    "은닉층을 추가하여 비선형 문제를 해결하는 MLP를 구현합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 다층 신경망 구조 이해\n",
    "- 역전파(Backpropagation) 알고리즘 이해\n",
    "- 활성화 함수의 역할 이해\n",
    "- MNIST 손글씨 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XOR 문제\n",
    "\n",
    "퍼셉트론의 한계를 보여주는 대표적인 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 데이터\n",
    "X_xor = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y_xor = torch.tensor([0, 1, 1, 0], dtype=torch.float32)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], c='blue', s=200, label='Class 0')\n",
    "plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], c='red', s=200, label='Class 1')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('XOR Problem - Not Linearly Separable')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 다층 퍼셉트론(MLP) 구조\n",
    "\n",
    "```\n",
    "입력층 → 은닉층 → 출력층\n",
    "  x    → h=σ(W₁x+b₁) → y=W₂h+b₂\n",
    "```\n",
    "\n",
    "은닉층의 비선형 활성화 함수가 핵심입니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.nn.layers import Linear\n",
    "from mlfs.nn.activations import relu, sigmoid\n",
    "from mlfs.nn.losses import BCELoss\n",
    "from mlfs.nn.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR을 위한 간단한 MLP\n",
    "class SimpleMLP:\n",
    "    def __init__(self):\n",
    "        self.fc1 = Linear(2, 4)  # 입력 → 은닉\n",
    "        self.fc2 = Linear(4, 1)  # 은닉 → 출력\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = relu(self.fc1(x))    # 은닉층 + ReLU\n",
    "        out = sigmoid(self.fc2(h))  # 출력층 + Sigmoid\n",
    "        return out.squeeze()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.fc1.parameters() + self.fc2.parameters()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 학습\n",
    "model = SimpleMLP()\n",
    "optimizer = Adam(model.parameters(), lr=0.1)\n",
    "criterion = BCELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    output = model(X_xor)\n",
    "    loss = criterion(output, y_xor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        predictions = (output > 0.5).float()\n",
    "        accuracy = (predictions == y_xor).float().mean()\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss.item():.4f}, Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인\n",
    "with torch.no_grad():\n",
    "    output = model(X_xor)\n",
    "    predictions = (output > 0.5).float()\n",
    "\n",
    "print(\"\\nXOR Results:\")\n",
    "for i in range(4):\n",
    "    print(f\"Input: {X_xor[i].tolist()}, Predicted: {predictions[i].item():.0f}, Actual: {y_xor[i].item():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MNIST 분류\n",
    "\n",
    "이제 실제 데이터셋인 MNIST를 분류해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.utils.data import load_mnist\n",
    "from mlfs.utils.viz import plot_images\n",
    "\n",
    "# 데이터 로드\n",
    "X_train, y_train = load_mnist(train=True, flatten=True)\n",
    "X_test, y_test = load_mnist(train=False, flatten=True)\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 이미지 확인\n",
    "X_sample, y_sample = load_mnist(train=True, flatten=False)\n",
    "plot_images(X_sample[:10], labels=y_sample[:10], n_rows=2, n_cols=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.nn.models import MLP\n",
    "from mlfs.nn.losses import CrossEntropyLoss\n",
    "\n",
    "# MLP 모델 생성: 784 → 256 → 128 → 10\n",
    "model = MLP([784, 256, 128, 10])\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "n_train = len(X_train)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    # 셔플\n",
    "    indices = torch.randperm(n_train)\n",
    "    \n",
    "    for i in range(0, n_train, batch_size):\n",
    "        batch_idx = indices[i:i+batch_size]\n",
    "        X_batch = X_train[batch_idx]\n",
    "        y_batch = y_train[batch_idx]\n",
    "        \n",
    "        # 순전파\n",
    "        logits = model(X_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch_idx)\n",
    "        correct += (logits.argmax(dim=1) == y_batch).sum().item()\n",
    "    \n",
    "    train_loss = epoch_loss / n_train\n",
    "    train_acc = correct / n_train\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # 테스트 정확도\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(X_test)\n",
    "        test_acc = (test_logits.argmax(dim=1) == y_test).float().mean().item()\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1:2d}: Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(train_losses)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(train_accs, label='Train')\n",
    "axes[1].plot(test_accs, label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 시각화\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model.predict(X_test[:20])\n",
    "\n",
    "X_test_img, _ = load_mnist(train=False, flatten=False)\n",
    "plot_images(X_test_img[:20], labels=y_test[:20], predictions=predictions, n_rows=4, n_cols=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "이 노트북에서 배운 내용:\n",
    "\n",
    "1. **MLP 구조**: 여러 층을 쌓아 비선형 문제 해결\n",
    "2. **활성화 함수**: ReLU, Sigmoid 등으로 비선형성 추가\n",
    "3. **역전파**: 체인룰을 사용한 그래디언트 계산\n",
    "4. **MNIST 분류**: ~97% 정확도 달성\n",
    "\n",
    "다음 노트북에서는 이미지에 더 적합한 **CNN**을 학습합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
