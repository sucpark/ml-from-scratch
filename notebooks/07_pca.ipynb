{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 주성분 분석 (PCA)\n",
    "\n",
    "차원 축소의 가장 기본적인 방법인 PCA를 구현합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 공분산 행렬과 고유값 분해 이해\n",
    "- 주성분의 의미 이해\n",
    "- 차원 축소 및 데이터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA 원리\n",
    "\n",
    "데이터의 분산을 최대화하는 방향(주성분)을 찾습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 상관 데이터 생성\n",
    "n_samples = 200\n",
    "X = torch.randn(n_samples, 2)\n",
    "# 상관관계 추가\n",
    "X[:, 1] = X[:, 0] * 0.8 + X[:, 1] * 0.3\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.7)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Correlated 2D Data')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.classical.reduction import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 적용\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "\n",
    "# 주성분 시각화\n",
    "mean = pca.mean_\n",
    "components = pca.components_\n",
    "explained_var = pca.explained_variance_\n",
    "\n",
    "print(f\"Explained Variance: {explained_var}\")\n",
    "print(f\"Explained Variance Ratio: {explained_var / explained_var.sum()}\")\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "\n",
    "# 주성분 화살표\n",
    "for i in range(2):\n",
    "    scale = torch.sqrt(explained_var[i]) * 2\n",
    "    plt.arrow(mean[0], mean[1], \n",
    "              components[i, 0] * scale, components[i, 1] * scale,\n",
    "              head_width=0.1, head_length=0.05, fc=f'C{i}', ec=f'C{i}',\n",
    "              label=f'PC{i+1}')\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Principal Components')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MNIST 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.utils.data import load_mnist_subset\n",
    "\n",
    "X_mnist, y_mnist = load_mnist_subset(n_samples=2000, flatten=True)\n",
    "print(f\"Original shape: {X_mnist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D로 축소\n",
    "pca_mnist = PCA(n_components=2)\n",
    "X_reduced = pca_mnist.fit_transform(X_mnist)\n",
    "\n",
    "print(f\"Reduced shape: {X_reduced.shape}\")\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "                      c=y_mnist, cmap='tab10', alpha=0.7, s=10)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('MNIST - PCA (2D)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 차원에서의 설명력\n",
    "pca_full = PCA(n_components=100)\n",
    "pca_full.fit(X_mnist)\n",
    "\n",
    "explained_ratio = pca_full.explained_variance_ / pca_full.explained_variance_.sum()\n",
    "cumulative_ratio = torch.cumsum(explained_ratio, dim=0)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 21), explained_ratio[:20].numpy())\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Individual Explained Variance')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 101), cumulative_ratio.numpy())\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95%')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 95% 설명력을 위한 성분 수\n",
    "n_95 = (cumulative_ratio >= 0.95).nonzero()[0, 0].item() + 1\n",
    "print(f\"\\n95% variance explained with {n_95} components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 재구성\n",
    "\n",
    "축소된 데이터에서 원본 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.utils.viz import plot_images\n",
    "from mlfs.utils.data import load_mnist\n",
    "\n",
    "# 원본 이미지\n",
    "X_img, _ = load_mnist(train=True, flatten=False)\n",
    "\n",
    "# 다양한 차원으로 축소 후 재구성\n",
    "n_components_list = [10, 50, 100, 200]\n",
    "\n",
    "fig, axes = plt.subplots(len(n_components_list) + 1, 5, figsize=(10, 10))\n",
    "\n",
    "# 원본\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow(X_img[i, 0], cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "\n",
    "# 재구성\n",
    "X_flat = X_mnist[:5]\n",
    "for row, n_comp in enumerate(n_components_list, 1):\n",
    "    pca = PCA(n_components=n_comp)\n",
    "    X_reduced = pca.fit_transform(X_flat)\n",
    "    X_reconstructed = pca.inverse_transform(X_reduced)\n",
    "    \n",
    "    for i in range(5):\n",
    "        img = X_reconstructed[i].reshape(28, 28)\n",
    "        axes[row, i].imshow(img, cmap='gray')\n",
    "        axes[row, i].axis('off')\n",
    "    axes[row, 0].set_ylabel(f'n={n_comp}', fontsize=12)\n",
    "\n",
    "plt.suptitle('PCA Reconstruction', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "1. **PCA**: 분산을 최대화하는 방향 찾기\n",
    "2. **고유값 분해**: 공분산 행렬의 고유벡터 = 주성분\n",
    "3. **차원 축소**: 중요한 성분만 유지\n",
    "4. **시각화**: 고차원 데이터를 2D/3D로 표현\n",
    "\n",
    "다음: 비선형 차원축소인 **LLE**와 **Isomap**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
