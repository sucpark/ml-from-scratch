{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 앙상블 학습\n",
    "\n",
    "여러 약한 분류기를 결합하는 AdaBoost를 구현합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 앙상블 학습의 원리 이해\n",
    "- AdaBoost 알고리즘 이해\n",
    "- 약한 분류기와 강한 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이진 분류 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 이진 분류 데이터\n",
    "n_samples = 200\n",
    "\n",
    "# 클래스 1\n",
    "X1 = torch.randn(n_samples // 2, 2) * 0.5 + torch.tensor([1.0, 1.0])\n",
    "# 클래스 -1\n",
    "X2 = torch.randn(n_samples // 2, 2) * 0.5 + torch.tensor([-1.0, -1.0])\n",
    "\n",
    "X = torch.cat([X1, X2], dim=0)\n",
    "y = torch.cat([torch.ones(n_samples // 2), -torch.ones(n_samples // 2)])\n",
    "\n",
    "# 섞기\n",
    "perm = torch.randperm(n_samples)\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Class +1', alpha=0.7)\n",
    "plt.scatter(X[y == -1, 0], X[y == -1, 1], c='red', label='Class -1', alpha=0.7)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Binary Classification Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 약한 분류기: Decision Stump\n",
    "\n",
    "결정 그루터기(Decision Stump)는 단일 특징과 임계값으로 분류하는 가장 간단한 분류기입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.classical.ensemble import DecisionStump\n",
    "\n",
    "# 단일 Decision Stump\n",
    "stump = DecisionStump()\n",
    "stump.fit(X, y)\n",
    "\n",
    "print(f\"Feature: {stump.feature_idx}\")\n",
    "print(f\"Threshold: {stump.threshold:.4f}\")\n",
    "print(f\"Polarity: {stump.polarity}\")\n",
    "\n",
    "# 예측\n",
    "pred = stump.predict(X)\n",
    "accuracy = (pred == y).float().mean()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaBoost 알고리즘\n",
    "\n",
    "1. 모든 샘플에 동일한 가중치 초기화\n",
    "2. 각 라운드에서:\n",
    "   - 가중치 기반으로 약한 분류기 학습\n",
    "   - 가중 에러 계산\n",
    "   - 분류기 가중치(α) 계산\n",
    "   - 샘플 가중치 업데이트 (잘못 분류된 샘플 가중치 증가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.classical.ensemble import AdaBoost\n",
    "\n",
    "# AdaBoost 학습\n",
    "ada = AdaBoost(n_estimators=10)\n",
    "ada.fit(X, y)\n",
    "\n",
    "print(f\"Number of stumps: {len(ada.stumps)}\")\n",
    "print(f\"\\nStump weights (alpha):\")\n",
    "for i, alpha in enumerate(ada.alphas[:5]):\n",
    "    print(f\"  Stump {i+1}: {alpha:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 확인\n",
    "pred = ada.predict(X)\n",
    "accuracy = (pred == y).float().mean()\n",
    "print(f\"AdaBoost Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 결정 경계 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title):\n",
    "    \"\"\"결정 경계 시각화\"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    xx, yy = torch.meshgrid(\n",
    "        torch.linspace(x_min, x_max, 100),\n",
    "        torch.linspace(y_min, y_max, 100),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    \n",
    "    grid = torch.stack([xx.flatten(), yy.flatten()], dim=1)\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdBu')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', label='Class +1', edgecolors='k')\n",
    "    plt.scatter(X[y == -1, 0], X[y == -1, 1], c='red', label='Class -1', edgecolors='k')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 단일 Stump vs AdaBoost\n",
    "plot_decision_boundary(stump, X, y, 'Single Decision Stump')\n",
    "plot_decision_boundary(ada, X, y, 'AdaBoost (10 Stumps)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 에스티메이터 수에 따른 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련/테스트 분할\n",
    "n_train = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]\n",
    "\n",
    "n_estimators_list = [1, 5, 10, 20, 50, 100]\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    model = AdaBoost(n_estimators=n_est)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_accs.append((train_pred == y_train).float().mean().item())\n",
    "    test_accs.append((test_pred == y_test).float().mean().item())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_list, train_accs, 'b-o', label='Train')\n",
    "plt.plot(n_estimators_list, test_accs, 'r-o', label='Test')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('AdaBoost: Effect of Number of Estimators')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 더 복잡한 데이터에서의 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR 패턴 데이터\n",
    "n = 100\n",
    "X_xor = torch.cat([\n",
    "    torch.randn(n, 2) * 0.3 + torch.tensor([0.5, 0.5]),\n",
    "    torch.randn(n, 2) * 0.3 + torch.tensor([-0.5, -0.5]),\n",
    "    torch.randn(n, 2) * 0.3 + torch.tensor([0.5, -0.5]),\n",
    "    torch.randn(n, 2) * 0.3 + torch.tensor([-0.5, 0.5])\n",
    "], dim=0)\n",
    "\n",
    "y_xor = torch.cat([\n",
    "    torch.ones(n), torch.ones(n), -torch.ones(n), -torch.ones(n)\n",
    "])\n",
    "\n",
    "# AdaBoost with many stumps\n",
    "ada_xor = AdaBoost(n_estimators=50)\n",
    "ada_xor.fit(X_xor, y_xor)\n",
    "\n",
    "plot_decision_boundary(ada_xor, X_xor, y_xor, 'AdaBoost on XOR Pattern (50 Stumps)')\n",
    "\n",
    "pred_xor = ada_xor.predict(X_xor)\n",
    "print(f\"XOR Accuracy: {(pred_xor == y_xor).float().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "1. **앙상블 학습**: 여러 약한 분류기를 결합하여 강한 분류기 생성\n",
    "2. **AdaBoost**: 순차적으로 학습, 잘못 분류된 샘플에 집중\n",
    "3. **Decision Stump**: 가장 간단한 약한 분류기 (단일 임계값)\n",
    "4. **장점**: 과적합에 강함, 복잡한 경계 학습 가능\n",
    "5. **핵심 수식**: α = 0.5 * log((1-ε)/ε), 가중치 업데이트\n",
    "\n",
    "다음: **얼굴 검출** (Haar-like 특징, Viola-Jones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
