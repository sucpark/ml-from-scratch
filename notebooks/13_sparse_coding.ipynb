{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. 희소 코딩 (Sparse Coding)\n",
    "\n",
    "데이터를 희소한 표현으로 분해하는 방법을 학습합니다.\n",
    "\n",
    "## 학습 목표\n",
    "- 희소 표현의 개념 이해\n",
    "- 딕셔너리 학습 (Dictionary Learning)\n",
    "- L1 정규화와 희소성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 희소 표현이란?\n",
    "\n",
    "신호 x를 딕셔너리 D의 열(atom)들의 선형 조합으로 표현:\n",
    "$$x \\approx D\\alpha$$\n",
    "\n",
    "여기서 α는 희소(sparse)해야 합니다 (대부분 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 예: 1D 신호\n",
    "n = 100  # 신호 길이\n",
    "k = 20   # 딕셔너리 atom 수\n",
    "\n",
    "# 기본 딕셔너리: DCT-like 기저\n",
    "D = torch.zeros(n, k)\n",
    "for i in range(k):\n",
    "    freq = (i + 1) * np.pi / n\n",
    "    D[:, i] = torch.cos(torch.arange(n).float() * freq)\n",
    "\n",
    "# 정규화\n",
    "D = D / D.norm(dim=0, keepdim=True)\n",
    "\n",
    "# 희소 계수로 신호 생성\n",
    "alpha_true = torch.zeros(k)\n",
    "alpha_true[2] = 1.5\n",
    "alpha_true[7] = -1.0\n",
    "alpha_true[15] = 0.8\n",
    "\n",
    "x = D @ alpha_true + torch.randn(n) * 0.1  # 노이즈 추가\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 원본 신호\n",
    "axes[0, 0].plot(x)\n",
    "axes[0, 0].set_title('Signal x')\n",
    "axes[0, 0].set_xlabel('Sample')\n",
    "\n",
    "# 딕셔너리 일부\n",
    "for i in range(5):\n",
    "    axes[0, 1].plot(D[:, i], label=f'Atom {i}')\n",
    "axes[0, 1].set_title('Dictionary Atoms (first 5)')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# True 희소 계수\n",
    "axes[1, 0].stem(alpha_true)\n",
    "axes[1, 0].set_title('True Sparse Coefficients α')\n",
    "axes[1, 0].set_xlabel('Atom index')\n",
    "\n",
    "# 재구성\n",
    "x_recon = D @ alpha_true\n",
    "axes[1, 1].plot(x, label='Original + noise')\n",
    "axes[1, 1].plot(x_recon, '--', label='Reconstruction')\n",
    "axes[1, 1].set_title('Reconstruction')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ISTA (Iterative Shrinkage-Thresholding Algorithm)\n",
    "\n",
    "희소 코딩 문제를 풀기 위한 알고리즘:\n",
    "$$\\min_\\alpha \\frac{1}{2}\\|x - D\\alpha\\|_2^2 + \\lambda\\|\\alpha\\|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(x, threshold):\n",
    "    \"\"\"Soft thresholding (shrinkage) 연산\"\"\"\n",
    "    return torch.sign(x) * torch.relu(torch.abs(x) - threshold)\n",
    "\n",
    "def ista(x, D, lambda_reg=0.1, max_iters=100):\n",
    "    \"\"\"ISTA 알고리즘\"\"\"\n",
    "    n, k = D.shape\n",
    "    alpha = torch.zeros(k)\n",
    "    \n",
    "    # 스텝 크기\n",
    "    L = torch.linalg.eigvalsh(D.T @ D).max()\n",
    "    step = 1.0 / L\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # Gradient step\n",
    "        grad = D.T @ (D @ alpha - x)\n",
    "        alpha = alpha - step * grad\n",
    "        \n",
    "        # Proximal (shrinkage) step\n",
    "        alpha = soft_threshold(alpha, lambda_reg * step)\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "# ISTA 실행\n",
    "alpha_recovered = ista(x, D, lambda_reg=0.1, max_iters=200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].stem(alpha_true, linefmt='b-', markerfmt='bo', label='True')\n",
    "axes[0].stem(alpha_recovered, linefmt='r--', markerfmt='rx', label='Recovered')\n",
    "axes[0].set_title('Sparse Coefficients')\n",
    "axes[0].legend()\n",
    "\n",
    "x_recon = D @ alpha_recovered\n",
    "axes[1].plot(x, label='Original')\n",
    "axes[1].plot(x_recon, '--', label='Reconstruction')\n",
    "axes[1].set_title(f'Reconstruction (MSE: {((x - x_recon)**2).mean():.4f})')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"True sparsity: {(alpha_true != 0).sum().item()}\")\n",
    "print(f\"Recovered sparsity: {(alpha_recovered.abs() > 0.01).sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 이미지 패치에서의 희소 코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 패치 생성\n",
    "patch_size = 8\n",
    "n_patches = 500\n",
    "\n",
    "# 간단한 패턴 생성\n",
    "patches = []\n",
    "for _ in range(n_patches):\n",
    "    patch = torch.zeros(patch_size, patch_size)\n",
    "    \n",
    "    # 랜덤 에지 또는 그라디언트\n",
    "    pattern = np.random.choice(['h_edge', 'v_edge', 'diag', 'spot'])\n",
    "    \n",
    "    if pattern == 'h_edge':\n",
    "        pos = np.random.randint(2, patch_size-2)\n",
    "        patch[:pos, :] = 1\n",
    "    elif pattern == 'v_edge':\n",
    "        pos = np.random.randint(2, patch_size-2)\n",
    "        patch[:, :pos] = 1\n",
    "    elif pattern == 'diag':\n",
    "        for i in range(patch_size):\n",
    "            patch[i, :i] = 1\n",
    "    else:  # spot\n",
    "        cx, cy = np.random.randint(1, patch_size-1, 2)\n",
    "        patch[cx-1:cx+2, cy-1:cy+2] = 1\n",
    "    \n",
    "    patch += torch.randn_like(patch) * 0.1\n",
    "    patches.append(patch.flatten())\n",
    "\n",
    "X = torch.stack(patches)  # (n_patches, patch_size^2)\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "\n",
    "# 샘플 패치 시각화\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(X[i].reshape(patch_size, patch_size), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Patches')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlfs.probabilistic.sparse import SparseCoding\n",
    "\n",
    "# 희소 코딩 학습\n",
    "n_atoms = 64\n",
    "sc = SparseCoding(n_atoms=n_atoms, lambda_reg=0.1, max_iters=50)\n",
    "sc.fit(X)\n",
    "\n",
    "print(f\"Dictionary shape: {sc.dictionary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 딕셔너리 시각화\n",
    "D = sc.dictionary\n",
    "\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < n_atoms:\n",
    "        atom = D[:, i].reshape(patch_size, patch_size)\n",
    "        ax.imshow(atom, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Learned Dictionary Atoms', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 재구성 품질"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 패치 재구성\n",
    "test_idx = [0, 10, 50, 100, 200]\n",
    "\n",
    "fig, axes = plt.subplots(len(test_idx), 3, figsize=(8, 12))\n",
    "\n",
    "for row, idx in enumerate(test_idx):\n",
    "    x = X[idx]\n",
    "    \n",
    "    # 희소 코드 계산\n",
    "    alpha = sc.encode(x.unsqueeze(0)).squeeze()\n",
    "    \n",
    "    # 재구성\n",
    "    x_recon = D @ alpha\n",
    "    \n",
    "    # 원본\n",
    "    axes[row, 0].imshow(x.reshape(patch_size, patch_size), cmap='gray')\n",
    "    axes[row, 0].set_title('Original' if row == 0 else '')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # 재구성\n",
    "    axes[row, 1].imshow(x_recon.reshape(patch_size, patch_size), cmap='gray')\n",
    "    axes[row, 1].set_title('Reconstructed' if row == 0 else '')\n",
    "    axes[row, 1].axis('off')\n",
    "    \n",
    "    # 희소 코드\n",
    "    axes[row, 2].stem(alpha.abs())\n",
    "    axes[row, 2].set_title(f'Sparse Code ({(alpha.abs() > 0.01).sum()} active)' if row == 0 else f'{(alpha.abs() > 0.01).sum()} active')\n",
    "    if row == len(test_idx) - 1:\n",
    "        axes[row, 2].set_xlabel('Atom index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 희소성과 재구성 품질 트레이드오프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 lambda 값으로 실험\n",
    "lambdas = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "test_x = X[0]\n",
    "\n",
    "fig, axes = plt.subplots(2, len(lambdas), figsize=(15, 6))\n",
    "\n",
    "for col, lam in enumerate(lambdas):\n",
    "    # 희소 코드 계산 (ISTA 직접 사용)\n",
    "    alpha = ista(test_x, D, lambda_reg=lam, max_iters=200)\n",
    "    x_recon = D @ alpha\n",
    "    \n",
    "    mse = ((test_x - x_recon)**2).mean().item()\n",
    "    sparsity = (alpha.abs() > 0.01).sum().item()\n",
    "    \n",
    "    # 재구성\n",
    "    axes[0, col].imshow(x_recon.reshape(patch_size, patch_size), cmap='gray')\n",
    "    axes[0, col].set_title(f'λ={lam}\\nMSE: {mse:.4f}')\n",
    "    axes[0, col].axis('off')\n",
    "    \n",
    "    # 희소 코드\n",
    "    axes[1, col].stem(alpha.abs())\n",
    "    axes[1, col].set_title(f'{sparsity} active atoms')\n",
    "\n",
    "axes[0, 0].set_ylabel('Reconstruction', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Sparse Code', fontsize=12)\n",
    "\n",
    "plt.suptitle('Effect of Regularization Strength', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "\n",
    "1. **희소 코딩**: 데이터를 소수의 기저 요소로 표현\n",
    "2. **L1 정규화**: 희소성 유도 (많은 계수가 0)\n",
    "3. **ISTA**: 근위 경사하강법으로 최적화\n",
    "4. **딕셔너리 학습**: 데이터에서 최적의 기저 학습\n",
    "5. **응용**: 이미지 압축, 노이즈 제거, 특징 학습\n",
    "\n",
    "## 축하합니다!\n",
    "\n",
    "이것으로 ml-from-scratch 튜토리얼을 모두 완료했습니다. 학습한 내용:\n",
    "- 신경망: Perceptron, MLP, CNN\n",
    "- 클러스터링: K-Means, Spectral Clustering\n",
    "- 차원 축소: PCA, LLE, Isomap\n",
    "- 앙상블: AdaBoost\n",
    "- 확률 모델: EM/GMM, Sparse Coding\n",
    "- 컴퓨터 비전: 얼굴 검출 개념"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
